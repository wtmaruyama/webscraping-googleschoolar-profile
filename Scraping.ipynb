{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen, urlretrieve\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://medium.com/ymedialabs-innovation/web-scraping-using-beautiful-soup-and-selenium-for-dynamic-page-2f8ad15efe25\n",
    "from selenium import webdriver\n",
    "\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('--ignore-certificate-errors')\n",
    "options.add_argument('--incognito')\n",
    "options.add_argument('--headless')\n",
    "driver = webdriver.Chrome(\"/usr/lib/chromium-browser/chromedriver\", options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://scholar.google.com/citations?user=9jjZobAAAAAJ&hl=en\"\n",
    "\n",
    "disabled = False\n",
    "driver.get(url)\n",
    "more_buttons = driver.find_elements_by_id(\"gsc_bpf_more\")#.find_elements_by_class_name(\"moreLink\")\n",
    "for bt in range(len(more_buttons)):\n",
    "    while(not disabled):\n",
    "        if more_buttons[bt].is_displayed():\n",
    "            driver.execute_script(\"arguments[0].click();\", more_buttons[bt])\n",
    "            time.sleep(1)\n",
    "            disabled = driver.execute_script(\"return arguments[0].hasAttribute(\\\"disabled\\\");\", more_buttons[bt]);\n",
    "page_source = driver.page_source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#page_source"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Obter html\n",
    "'''\n",
    "url = \"https://scholar.google.com/citations?user=9jjZobAAAAAJ&hl=en\"\n",
    "response = urlopen(url)\n",
    "html = response.read()#.decode(\"utf-8\")\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "name = soup.find(\"div\",{'id': 'gsc_prf_in'}).text\n",
    "table = soup.find_all(\"table\")[1]\n",
    "rows = table.find('tbody').find_all('tr')\n",
    "\n",
    "links = []\n",
    "citacoes = []\n",
    "anos = []\n",
    "for row in rows:\n",
    "    tds = row.find_all('td')\n",
    "    links.append(tds[0].find('a')['data-href'])\n",
    "    citacoes.append(tds[1].find('a').text)\n",
    "    anos.append(tds[2].text)\n",
    "    #citacoes.append(tds[0].find('a', {\"class\":\"gsc_a_ac gs_ibl\"}).text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nome: Luciano Antonio Digiampietri\n",
      "Quantidade de publicações: 168\n"
     ]
    }
   ],
   "source": [
    "print(\"Nome:\", name)\n",
    "print(\"Quantidade de publicações:\",len(links))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9jjZobAAAAAJ&citation_for_view=9jjZobAAAAAJ:u-x6o8ySG0sC\n"
     ]
    }
   ],
   "source": [
    "# Seleciono apenas o primeiro para não bloquear\n",
    "\n",
    "url = \"https://scholar.google.com\"+links[0]\n",
    "print(url)\n",
    "response = urlopen(url)\n",
    "html = response.read()    \n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Título:  The first provenance challenge\n",
      "Citacoes: 272\n",
      "Ano: 2008\n",
      "Authors : Luc Moreau, Bertram Ludäscher, Ilkay Altintas, Roger S Barga, Shawn Bowers, Steven Callahan, George Chin Jr, Ben Clifford, Shirley Cohen, Sarah Cohen‐Boulakia, Susan Davidson, Ewa Deelman, Luciano Digiampietri, Ian Foster, Juliana Freire, James Frew, Joe Futrelle, Tara Gibson, Yolanda Gil, Carole Goble, Jennifer Golbeck, Paul Groth, David A Holland, Sheng Jiang, Jihie Kim, David Koop, Ales Krenek, Timothy McPhillips, Gaurang Mehta, Simon Miles, Dominic Metzger, Steve Munroe, Jim Myers, Beth Plale, Norbert Podhorszki, Varun Ratnakar, Emanuele Santos, Carlos Scheidegger, Karen Schuchardt, Margo Seltzer, Yogesh L Simmhan, Claudio Silva, Peter Slaughter, Eric Stephan, Robert Stevens, Daniele Turi, Huy Vo, Mike Wilde, Jun Zhao, Yong Zhao\n",
      "Publication date : 2008/4/10\n",
      "Source : Concurrency and computation: practice and experience\n",
      "Volume : 20\n",
      "Issue : 5\n",
      "Pages : 409-418\n",
      "Publisher : John Wiley & Sons, Ltd.\n",
      "Description :  The first Provenance Challenge was set up in order to provide a forum for the community to understand the capabilities of different provenance systems and the expressiveness of their provenance representations. To this end, a functional magnetic resonance imaging workflow was defined, which participants had to either simulate or run in order to produce some provenance representation, from which a set of identified queries had to be implemented and executed. Sixteen teams responded to the challenge, and submitted their inputs. In this paper, we present the challenge workflow and queries, and summarize the participants' contributions. Copyright © 2007 John Wiley & Sons, Ltd.\n"
     ]
    }
   ],
   "source": [
    "# Baixar as informações do primeiro link\n",
    "title = soup.find(\"a\", {\"class\": \"gsc_vcd_title_link\"}).text\n",
    "print(\"Title: \", title)\n",
    "print(\"Cite:\", citacoes[0])\n",
    "print(\"Year:\", anos[0])\n",
    "\n",
    "fields = soup.findAll('div', {'class':'gsc_vcd_field'}, recursive=True)\n",
    "#print(fields)\n",
    "values = soup.findAll('div', {'class':'gsc_vcd_value'}, recursive=True)\n",
    "#print(values)\n",
    "\n",
    "data = {}\n",
    "for n in range(len(fields)-2):\n",
    "    print(fields[n].text, \":\", values[n].text)\n",
    "    #data[fields[n].text] = values[n].text\n",
    "    #print(values[n].text+'\\n\\n')\n",
    "    \n",
    "#print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
